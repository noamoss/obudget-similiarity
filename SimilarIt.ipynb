{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row0_col0 {\n",
       "            background-color:  #589ec8;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row0_col1 {\n",
       "            background-color:  #589ec8;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row0_col2 {\n",
       "            background-color:  #fff7fb;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row0_col3 {\n",
       "            background-color:  #589ec8;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row0_col4 {\n",
       "            background-color:  #589ec8;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row0_col5 {\n",
       "            background-color:  #fff7fb;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row0_col6 {\n",
       "            background-color:  #c6cce3;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row1_col0 {\n",
       "            background-color:  #589ec8;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row1_col1 {\n",
       "            background-color:  #589ec8;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row1_col2 {\n",
       "            background-color:  #c6cce3;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row1_col3 {\n",
       "            background-color:  #67a4cc;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row1_col4 {\n",
       "            background-color:  #67a4cc;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row1_col5 {\n",
       "            background-color:  #cacee5;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row1_col6 {\n",
       "            background-color:  #f2ecf5;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row2_col0 {\n",
       "            background-color:  #c6cce3;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row2_col1 {\n",
       "            background-color:  #c6cce3;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row2_col2 {\n",
       "            background-color:  #83afd3;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row2_col3 {\n",
       "            background-color:  #b9c6e0;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row2_col4 {\n",
       "            background-color:  #b9c6e0;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row2_col5 {\n",
       "            background-color:  #81aed2;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row2_col6 {\n",
       "            background-color:  #589ec8;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row3_col0 {\n",
       "            background-color:  #fff7fb;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row3_col1 {\n",
       "            background-color:  #fff7fb;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row3_col2 {\n",
       "            background-color:  #589ec8;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row3_col3 {\n",
       "            background-color:  #fff7fb;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row3_col4 {\n",
       "            background-color:  #fff7fb;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row3_col5 {\n",
       "            background-color:  #589ec8;\n",
       "        }    #T_fa333ba0_4efe_11e8_9226_f40669842fd8row3_col6 {\n",
       "            background-color:  #fff7fb;\n",
       "        }</style>  \n",
       "<table id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >abs_freq</th> \n",
       "        <th class=\"col_heading level0 col1\" >abs_perc</th> \n",
       "        <th class=\"col_heading level0 col2\" >abs_perc_cum</th> \n",
       "        <th class=\"col_heading level0 col3\" >wtd_freq</th> \n",
       "        <th class=\"col_heading level0 col4\" >wtd_freq_perc</th> \n",
       "        <th class=\"col_heading level0 col5\" >wtd_freq_perc_cum</th> \n",
       "        <th class=\"col_heading level0 col6\" >rel_value</th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8\" class=\"row_heading level0 row0\" >spain</th> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row0_col0\" class=\"data row0 col0\" >3</td> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row0_col1\" class=\"data row0 col1\" >0.333333</td> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row0_col2\" class=\"data row0 col2\" >0.333333</td> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row0_col3\" class=\"data row0 col3\" >510</td> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row0_col4\" class=\"data row0 col4\" >0.335526</td> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row0_col5\" class=\"data row0 col5\" >0.335526</td> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row0_col6\" class=\"data row0 col6\" >170</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8\" class=\"row_heading level0 row1\" >beaches</th> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row1_col0\" class=\"data row1 col0\" >3</td> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row1_col1\" class=\"data row1 col1\" >0.333333</td> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row1_col2\" class=\"data row1 col2\" >0.666667</td> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row1_col3\" class=\"data row1 col3\" >490</td> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row1_col4\" class=\"data row1 col4\" >0.322368</td> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row1_col5\" class=\"data row1 col5\" >0.657895</td> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row1_col6\" class=\"data row1 col6\" >163</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8\" class=\"row_heading level0 row2\" >france</th> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row2_col0\" class=\"data row2 col0\" >2</td> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row2_col1\" class=\"data row2 col1\" >0.222222</td> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row2_col2\" class=\"data row2 col2\" >0.888889</td> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row2_col3\" class=\"data row2 col3\" >360</td> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row2_col4\" class=\"data row2 col4\" >0.236842</td> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row2_col5\" class=\"data row2 col5\" >0.894737</td> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row2_col6\" class=\"data row2 col6\" >180</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8\" class=\"row_heading level0 row3\" >best</th> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row3_col0\" class=\"data row3 col0\" >1</td> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row3_col1\" class=\"data row3 col1\" >0.111111</td> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row3_col2\" class=\"data row3 col2\" >1</td> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row3_col3\" class=\"data row3 col3\" >160</td> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row3_col4\" class=\"data row3 col4\" >0.105263</td> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row3_col5\" class=\"data row3 col5\" >1</td> \n",
       "        <td id=\"T_fa333ba0_4efe_11e8_9226_f40669842fd8row3_col6\" class=\"data row3 col6\" >160</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fb2adc02278>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# default value\n",
    "\n",
    "word_freq = defaultdict(lambda: [0,0])\n",
    "\n",
    "# how many times a word appears in the corpus\n",
    "num_list = [200, 180, 170, 160,160]\n",
    "\n",
    "# text\n",
    "text_list = ['france', 'spain', 'spain beaches', 'france beaches', 'spain best beaches']\n",
    "\n",
    "\n",
    "\n",
    "# loop over the text and the number\n",
    "for text, num in zip(text_list, num_list):\n",
    "    for word in text.split():\n",
    "        word_freq[word][0] += 1\n",
    "        word_freq[word][1] += num\n",
    "\n",
    "\n",
    "columns = {0: 'abs_freq', 1:'wtd_freq'}\n",
    "\n",
    "abs_wtd_df = pd.DataFrame.from_dict(word_freq,orient='index')\\\n",
    "            .rename(columns=columns) \\\n",
    "            .sort_values('wtd_freq', ascending=False) \\\n",
    "            .assign(rel_value=lambda df: df['wtd_freq'] / df['abs_freq'])\\\n",
    "            .round()\n",
    "\n",
    "\n",
    "abs_wtd_df.insert(1, 'abs_perc', value=abs_wtd_df['abs_freq']/abs_wtd_df['abs_freq'].sum())\n",
    "abs_wtd_df.insert(2, 'abs_perc_cum', abs_wtd_df['abs_perc'].cumsum())\n",
    "abs_wtd_df.insert(4, 'wtd_freq_perc', abs_wtd_df['wtd_freq'] / abs_wtd_df['wtd_freq'].sum())\n",
    "abs_wtd_df.insert(5, 'wtd_freq_perc_cum', abs_wtd_df['wtd_freq_perc'].cumsum())\n",
    "abs_wtd_df.style.background_gradient(low=0, high=.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloaded:  10000  items\n",
      "downloaded:  20000  items\n",
      "downloaded:  30000  items\n",
      "downloaded:  40000  items\n",
      "downloaded:  50000  items\n",
      "downloaded:  60000  items\n",
      "downloaded:  70000  items\n",
      "downloaded:  80000  items\n",
      "downloaded:  90000  items\n",
      "downloaded:  100000  items\n",
      "downloaded:  110000  items\n",
      "downloaded:  117414  items\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "### load a datapackage and transform into a pandas dataframe \n",
    "\n",
    "import datapackage\n",
    "\n",
    "package=datapackage.Package('http://next.obudget.org/datapackages/procurement/tenders/processed/datapackage.json')\n",
    "response = package.resources[0]\n",
    "iterator = response.iter(keyed=True)\n",
    "\n",
    "df=pd.DataFrame()\n",
    "\n",
    "items = []\n",
    "counter = 0\n",
    "for row in iterator:\n",
    "    for column_name in list(row.keys()):\n",
    "        if column_name not in df.columns:\n",
    "            df.insert(column=column_name,loc=len(df.columns),value=None)\n",
    "    items.append(row)\n",
    "    counter+=1\n",
    "    if counter % 10000 == 0:\n",
    "        print(\"downloaded: \",counter,\" items\")\n",
    "print(\"downloaded: \",counter,\" items\")\n",
    "\n",
    "df = pd.DataFrame(items)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import json, re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "# add a row index to df\n",
    "df['doc_index'] = df.index    \n",
    "\n",
    "# get rid of non alpha numberics, split to words in selected columns\n",
    "def tokenize(row):\n",
    "    columns_list = (['description','entity_id']) \n",
    "    tokenized = []\n",
    "    for column_name in columns_list:\n",
    "        tokenized+=tokenize_text(row[column_name])\n",
    "    return \", \".join(tokenized)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    regxlst = [re.compile(x) for x in [r'\\d+',]]    # regular expressions list for clean ups: \n",
    "                                                             # 1. only digits    \n",
    "    stringed_text = str(text)\n",
    "    ignore_signs = [\"\\,\",\"\\:\",\"\\;\",\"\\.\",\"\\&\",\"\\$\",\"\\-\",\"\\=\",\"\\(\",\"\\)\",\"\\d+\",\"\\\\n\"]\n",
    "    cleaned_1 = re.sub(\"|\".join(ignore_signs),\"\",stringed_text) # remove non-alphanumberic characters\n",
    "    cleaned_2 = re.sub(\"  \",\" \",cleaned_1)                # no more double spaces\n",
    "    cleaned_3 = cleaned_2.split(\" \")                       # split into separate words list\n",
    "    cleaned_4 =  [v for regex in regxlst for v in set(cleaned_3) if not(regex.match(v)) and not cleaned_3.remove(v)] # filter by regular expressions\n",
    "    cleaned_5 = [word for word in cleaned_4 if word is not None]\n",
    "    return cleaned_4\n",
    "\n",
    "# creata words list per doc, add to Dataframe ('tokenized')\n",
    "\n",
    "df['tokenized'] = df.apply(tokenize,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tfidf \n",
    "import operator\n",
    "\n",
    "\n",
    "def create_words_index(texts_list):  # create words counter index\n",
    "    def defaultvalue():   # set word counter to 0 \n",
    "        return 0\n",
    "    \n",
    "    words_index = defaultdict(defaultvalue) # create an index with 0 as default\n",
    "    \n",
    "    for text in texts_list:\n",
    "        words = text.split(\", \")\n",
    "        for word in words:\n",
    "            words_index[word] += 1\n",
    "    return words_index\n",
    "\n",
    "# create a words counter index\n",
    "data = df['tokenized'] \n",
    "words_index = create_words_index(data)\n",
    "\n",
    "words_list = sorted(words_index.items(), key=operator.itemgetter(1), reverse=True) # words list by frequency order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-876dfbba63b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create a dataframe with words as features, each row is a doc vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mterm_in_doc_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwords_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcount_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/newone/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    347\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer_dtype_from_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 mgr = self._init_ndarray(values, index, columns, dtype=dtype,\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create a dataframe with words as features, each row is a doc vector\n",
    "term_in_doc_index = pd.DataFrame(0, index=np.arange(len(df)), columns=words_index.keys())\n",
    "\n",
    "def count_word(l, word):\n",
    "    return l.count(word)\n",
    "\n",
    "# count terms in corpus on index (terms_index)\n",
    "for word in term_in_doc_index.keys():\n",
    "    term_in_doc_index[word] = df['tokenized'].apply(count_word, args=[word])\n",
    "    \n",
    "    \n",
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "tfidf.fit(terms_index)\n",
    "\n",
    "tf_idf_matrix = tfidf.transform(terms_index)\n",
    "\n",
    "def similiar_five_docs (row):  # check the 5 most similiar texts to the doc\n",
    "    doc_index = row['doc_index']\n",
    "    cosines = cosine_similarity(tf_idf_matrix[doc_index:doc_index+1], tf_idf_matrix)\n",
    "    return list(np.argsort(-cosines, axis=1)[0][1:6]) #exclude the current doc from the results list\n",
    "\n",
    "# add a a new column with a reference (row num) of similar 5 items \n",
    "df['similar_docs'] = df.apply(similiar_five_docs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initials_clearance(words):                             # unify counts of returnning terms with initials\n",
    "    initials = [\"ה\",\"ו\",\"י\",\"ב\",\"כ\",\"ל\",\"מ\",\"וב\",\"ול\",\"ומ\",\"ש\"]\n",
    "    words_corpus = list(words_index.keys())\n",
    "\n",
    "    def with_initials(word):                            # create list of intitial+word combinations\n",
    "        combinations_to_check = []\n",
    "        for initial in initials:\n",
    "            combinations_to_check.append(initial+word)\n",
    "        return combinations_to_check\n",
    "\n",
    "    def without_initials(word):                         # create a list of word witout initial combinations\n",
    "        combinations_to_check = []\n",
    "        for initial in initials:\n",
    "            if len(word) > 2 and word[0] == initial:\n",
    "                combinations_to_check.append(word[1:])\n",
    "        return combinations_to_check\n",
    "\n",
    "    \n",
    "\n",
    "    for word in words:\n",
    "        justify_unification = [word]\n",
    "\n",
    "        if words_index[word] > 0 :\n",
    "                combinations_to_check = with_initials(word)     # add word+initials to combinations list  \n",
    "                combinations_to_check += without_initials(word)    # add word - initials to combinations list\n",
    "\n",
    "                for combination in combinations_to_check:\n",
    "                    if combination in words_corpus:\n",
    "                        justify_unification.append(combination)\n",
    "        \n",
    "        if len(justify_unification) > 2:     # we will unifiy key-values only if we have two exmaples of initial+word in the corpus\n",
    "\n",
    "            justify_unification.sort(key = len)\n",
    "            new_entry = justify_unification[0]    # the shortest combination (hopefully, the root word, to be used in the index)\n",
    "            \n",
    "            for combination in justify_unification:\n",
    "                words_index[new_entry] += words_index[combination]\n",
    "                words_index.pop(combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  93478\n"
     ]
    }
   ],
   "source": [
    "words = list(words_index.keys())\n",
    "print(\"before: \",len(words))\n",
    "initials_clearance(words)\n",
    "print(\"after: \",len(words_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_signs = [\"\\,\",\"\\:\",\"\\;\",\"\\.\",\"\\&\",\"\\$\",\"\\-\",\"\\=\",\"\\(\",\"\\)\",\"\\d+\",\"\\\\n\"]\n",
    "\"|\".join(ignore_signs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
